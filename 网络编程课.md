# TCP性能测试

tcp性能测试关注的指标：

1. 带宽：只管数据量，不管消息量。

   测试收发数据：nc&dd

   netcat(nc)是一个简单而有用的工具，被誉为网络安全界的“瑞士军刀”，不仅可以通过使用TCP或UDP协议的网络连接读写数据，同时还是一个功能强大的网络调试和探测工具，能够建立你需要的几乎所有类型的网络连接。

   dd是用来复制文件的命令，它可以用指定大小的数据块复制一个文件，并在复制的同时进行指定的转换。

   dd选项:

   - if=输入文件：指定源文件或源设备；
   - of=输出文件：指定目标文件或目标设备；
   - bs=字节数：指定一次输入/输出多少字节，即把这些字节看作一个数据块；
   - count=个数：指定输入/输出多少个数据块；

   ```shell
   # 开启服务器连接
   nc -lk 8888
   # 测试
   netstat -anop |grep 8888        # 在连接之前查看端口是否存在
   nc localhost 8888　　　　　　　　# 连接端口进行聊天
   
   
   # 5001端口收数据，转发到黑洞
   nc -l 5001 > /dev/null
   # 从dev/zero读1000个1MB数据，发送到e6400的5001端口
   dd if=/dev/zero bs=1MB count=1000 |nc e6400 5001
   # dd会打印出带宽
   
   # 本机测试
   dd if=/dev/zero bs=1MB count=10000 |nc localhost 5001
   
   # 时间.从磁盘读
   time nc localhost 5001 < filename
   
   # pv：显示接收速度。四个进程两个核，速度有影响，争用
   nc -l 5001 | pv -W > /dev/null
   dd if=/dev/zero bs=1MB count=10000 |nc localhost 5001
   ```

   可以用top来看每个进程占用CPU的情况。

   其他消息发送要分包，测出来的带宽会小点。如果测出来的比这个大说明进行了压缩。如果小太多，CPU使用率很低，说明没有利用好网络带宽和CPU带宽。如果CPU使用率高，说明计算开销大。比如ssh拷文件，ssh只能用单核单线程，多核也不能加锁。尝试把ssh加密并行化来提高效率，但是不一定有意义。

2. 吞吐量

   应用层，一秒钟能处理多少条消息，执行多少次查询，完成多少次事务。例如QPS, TPS

3. 延迟

   平均延迟，百分数延迟。

4. 资源使用率(比例)

   效率

5. CPU使用率

6. 额外开销。例如两台机器直接拷文件CPU利用率很低。如果事实压缩加密，CPU利用率会很高。能不能提高效率取决于CPU资源的大小。要先压缩后加密。压缩和拷贝可以重叠，流水线操作有可能把时间加快。可以提高拷文件效率，代价是CPU使用率高。可以估算一下是否有意义。

# TTCP

协议是有格式的。echo是收到啥发啥，不用管tcp分包。是一个用tcp实现的程序。发一个数据等一个相应。测出来的性能有意义。还可以观察不同语言的性能。一个client，一个server，没有并发连接。

## 用到的协议

请求响应协议

cli告诉ser要发送多少数据SessionMessage->cli开始发送数据(头+运行时决定的长度)(会一直等回复)PayloadMessage->ser回复收到数据的响应ACK->重复->ser收到最后一个后，发送完ack就断开连接。

如果带宽大延迟长，网络使用率就不如nc这种没头脑的发送方式。

例子：都是不支持并发连接。每个连接启动一个线程就支持并发了。

[C语言实现](muduo\examples\ace\ttcp\ttcp_blocking.cc)

```c
#include "examples/ace/ttcp/common.h"
#include "muduo/base/Timestamp.h"
#include "muduo/base/Types.h"

#undef NDEBUG

#include <assert.h>
#include <errno.h>
#include <stdio.h>
#include <unistd.h>

#include <netinet/in.h>
#include <arpa/inet.h>

static int acceptOrDie(uint16_t port)
{
  int listenfd = ::socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
  assert(listenfd >= 0);

  int yes = 1;
  if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &yes, sizeof(yes)))
  {
    perror("setsockopt");
    exit(1);
  }

  struct sockaddr_in addr;
  muduo::memZero(&addr, sizeof(addr));
  addr.sin_family = AF_INET;
  addr.sin_port = htons(port);
  addr.sin_addr.s_addr = INADDR_ANY;
  if (bind(listenfd, reinterpret_cast<struct sockaddr*>(&addr), sizeof(addr)))
  {
    perror("bind");
    exit(1);
  }

  if (listen(listenfd, 5))
  {
    perror("listen");
    exit(1);
  }

  struct sockaddr_in peer_addr;
  muduo::memZero(&peer_addr, sizeof(peer_addr));
  socklen_t addrlen = 0;
  int sockfd = ::accept(listenfd, reinterpret_cast<struct sockaddr*>(&peer_addr), &addrlen);
  if (sockfd < 0)
  {
    perror("accept");
    exit(1);
  }
  ::close(listenfd);
  return sockfd;
}

static int write_n(int sockfd, const void* buf, int length)
{
  int written = 0;
  while (written < length)
  {
    ssize_t nw = ::write(sockfd, static_cast<const char*>(buf) + written, length - written);
    if (nw > 0)
    {
      written += static_cast<int>(nw);
    }
    else if (nw == 0)
    {
      break;  // EOF
    }
    else if (errno != EINTR)
    {
      perror("write");
      break;
    }
  }
  return written;
}

static int read_n(int sockfd, void* buf, int length)
{
  int nread = 0;
  while (nread < length)
  {
    ssize_t nr = ::read(sockfd, static_cast<char*>(buf) + nread, length - nread);
    if (nr > 0)
    {
      nread += static_cast<int>(nr);
    }
    else if (nr == 0)
    {
      break;  // EOF
    }
    else if (errno != EINTR)
    {
      perror("read");
      break;
    }
  }
  return nread;
}

void transmit(const Options& opt)
{
  struct sockaddr_in addr = resolveOrDie(opt.host.c_str(), opt.port);
  printf("connecting to %s:%d\n", inet_ntoa(addr.sin_addr), opt.port);

  int sockfd = ::socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
  assert(sockfd >= 0);
  int ret = ::connect(sockfd, reinterpret_cast<struct sockaddr*>(&addr), sizeof(addr));
  if (ret)
  {
    perror("connect");
    printf("Unable to connect %s\n", opt.host.c_str());
    ::close(sockfd);
    return;
  }

  printf("connected\n");
  muduo::Timestamp start(muduo::Timestamp::now());
  struct SessionMessage sessionMessage = { 0, 0 };
  sessionMessage.number = htonl(opt.number);
  sessionMessage.length = htonl(opt.length);
  if (write_n(sockfd, &sessionMessage, sizeof(sessionMessage)) != sizeof(sessionMessage))
  {
    perror("write SessionMessage");
    exit(1);
  }

  const int total_len = static_cast<int>(sizeof(int32_t) + opt.length);
  PayloadMessage* payload = static_cast<PayloadMessage*>(::malloc(total_len));
  assert(payload);
  payload->length = htonl(opt.length);
  for (int i = 0; i < opt.length; ++i)
  {
    payload->data[i] = "0123456789ABCDEF"[i % 16];
  }

  double total_mb = 1.0 * opt.length * opt.number / 1024 / 1024;
  printf("%.3f MiB in total\n", total_mb);

  for (int i = 0; i < opt.number; ++i)
  {
    int nw = write_n(sockfd, payload, total_len);
    assert(nw == total_len);

    int ack = 0;
    int nr = read_n(sockfd, &ack, sizeof(ack));
    assert(nr == sizeof(ack));
    ack = ntohl(ack);
    assert(ack == opt.length);
  }

  ::free(payload);
  ::close(sockfd);
  double elapsed = timeDifference(muduo::Timestamp::now(), start);
  printf("%.3f seconds\n%.3f MiB/s\n", elapsed, total_mb / elapsed);
}

void receive(const Options& opt)
{
  int sockfd = acceptOrDie(opt.port);
  // 读取。如果长度不一样说明出错
  struct SessionMessage sessionMessage = { 0, 0 };
  if (read_n(sockfd, &sessionMessage, sizeof(sessionMessage)) != sizeof(sessionMessage))
  {
    perror("read SessionMessage");
    exit(1);
  }
  // 字节序转换
  sessionMessage.number = ntohl(sessionMessage.number);
  sessionMessage.length = ntohl(sessionMessage.length);
    // 预计接收的数据长度
  printf("receive number = %d\nreceive length = %d\n",
         sessionMessage.number, sessionMessage.length);
    // 准备一块缓冲区接收消息
  const int total_len = static_cast<int>(sizeof(int32_t) + sessionMessage.length);
    // 如果对方故意发送一个很大的长度，malloc会被聚集响应攻击。可以给lenth设置最大长度。这个结构体的长度是运行时分配的
  PayloadMessage* payload = static_cast<PayloadMessage*>(::malloc(total_len));
  assert(payload);

  for (int i = 0; i < sessionMessage.number; ++i)
  {
    payload->length = 0;
    if (read_n(sockfd, &payload->length, sizeof(payload->length)) != sizeof(payload->length))
    {
      perror("read length");
      exit(1);
    }
    payload->length = ntohl(payload->length);
      // 确定长度，读取到data
    assert(payload->length == sessionMessage.length);
    if (read_n(sockfd, payload->data, payload->length) != payload->length)
    {
      perror("read payload data");
      exit(1);
    }
    // 构造一个响应
    int32_t ack = htonl(payload->length);
    if (write_n(sockfd, &ack, sizeof(ack)) != sizeof(ack))
    {
      perror("write ack");
      exit(1);
    }
  }
    // 释放内存，关闭连接
  ::free(payload);
  ::close(sockfd);
}
```

```c++
#include "examples/ace/ttcp/common.h"

#include "muduo/base/Logging.h"
#include "muduo/net/EventLoop.h"
#include "muduo/net/TcpClient.h"
#include "muduo/net/TcpServer.h"

#include <stdio.h>

using namespace muduo;
using namespace muduo::net;

EventLoop* g_loop;

struct Context
{
  int count;
  int64_t bytes;
  SessionMessage session;
  Buffer output;

  Context()
    : count(0),
      bytes(0)
  {
    session.number = 0;
    session.length = 0;
  }
};

/////////////////////////////////////////////////////////////////////
// T R A N S M I T
/////////////////////////////////////////////////////////////////////

namespace trans
{

void onConnection(const Options& opt, const TcpConnectionPtr& conn)
{
  if (conn->connected())
  {
    printf("connected\n");
    Context context;
    context.count = 1;
    context.bytes = opt.length;
    context.session.number = opt.number;
    context.session.length = opt.length;
    context.output.appendInt32(opt.length);
    context.output.ensureWritableBytes(opt.length);
    for (int i = 0; i < opt.length; ++i)
    {
      context.output.beginWrite()[i] = "0123456789ABCDEF"[i % 16];
    }
    context.output.hasWritten(opt.length);
    conn->setContext(context);

    SessionMessage sessionMessage = { 0, 0 };
    sessionMessage.number = htonl(opt.number);
    sessionMessage.length = htonl(opt.length);
    conn->send(&sessionMessage, sizeof(sessionMessage));

    conn->send(context.output.toStringPiece());
  }
  else
  {
    const Context& context = boost::any_cast<Context>(conn->getContext());
    LOG_INFO << "payload bytes " << context.bytes;
    conn->getLoop()->quit();
  }
}

void onMessage(const TcpConnectionPtr& conn, Buffer* buf, Timestamp time)
{
  Context* context = boost::any_cast<Context>(conn->getMutableContext());
  while (buf->readableBytes() >= sizeof(int32_t))
  {
    int32_t length = buf->readInt32();
    if (length == context->session.length)
    {
      if (context->count < context->session.number)
      {
        conn->send(context->output.toStringPiece());
        ++context->count;
        context->bytes += length;
      }
      else
      {
        conn->shutdown();
        break;
      }
    }
    else
    {
      conn->shutdown();
      break;
    }
  }
}

}  // namespace trans

void transmit(const Options& opt)
{
  InetAddress addr(opt.port);
  if (!InetAddress::resolve(opt.host, &addr))
  {
    LOG_FATAL << "Unable to resolve " << opt.host;
  }
  muduo::Timestamp start(muduo::Timestamp::now());
  EventLoop loop;
  g_loop = &loop;
  TcpClient client(&loop, addr, "TtcpClient");
  client.setConnectionCallback(
      std::bind(&trans::onConnection, opt, _1));
  client.setMessageCallback(
      std::bind(&trans::onMessage, _1, _2, _3));
  client.connect();
  loop.loop();
  double elapsed = timeDifference(muduo::Timestamp::now(), start);
  double total_mb = 1.0 * opt.length * opt.number / 1024 / 1024;
  printf("%.3f MiB transferred\n%.3f MiB/s\n", total_mb, total_mb / elapsed);
}

/////////////////////////////////////////////////////////////////////
// R E C E I V E
/////////////////////////////////////////////////////////////////////

namespace receiving
{

void onConnection(const TcpConnectionPtr& conn)
{
  if (conn->connected())
  {
    Context context;
    conn->setContext(context);
  }
  else
  {
    const Context& context = boost::any_cast<Context>(conn->getContext());
    LOG_INFO << "payload bytes " << context.bytes;
    conn->getLoop()->quit();
  }
}

void onMessage(const TcpConnectionPtr& conn, Buffer* buf, Timestamp time)
{
  while (buf->readableBytes() >= sizeof(int32_t))
  {
    Context* context = boost::any_cast<Context>(conn->getMutableContext());
    SessionMessage& session = context->session;
    if (session.number == 0 && session.length == 0)
    {
      if (buf->readableBytes() >= sizeof(SessionMessage))
      {
        session.number = buf->readInt32();
        session.length = buf->readInt32();
        context->output.appendInt32(session.length);
        printf("receive number = %d\nreceive length = %d\n",
               session.number, session.length);
      }
      else
      {
        break;
      }
    }
    else
    {
      const unsigned total_len = session.length + static_cast<int>(sizeof(int32_t));
      const int32_t length = buf->peekInt32();
      if (length == session.length)
      {
        if (buf->readableBytes() >= total_len)
        {
          buf->retrieve(total_len);
          conn->send(context->output.toStringPiece());
          ++context->count;
          context->bytes += length;
          if (context->count >= session.number)
          {
            conn->shutdown();
            break;
          }
        }
        else
        {
          break;
        }
      }
      else
      {
        printf("wrong length %d\n", length);
        conn->shutdown();
        break;
      }
    }
  }
}

}  // namespace receiving

void receive(const Options& opt)
{
  EventLoop loop;
  g_loop = &loop;
  InetAddress listenAddr(opt.port);
  TcpServer server(&loop, listenAddr, "TtcpReceive");
  server.setConnectionCallback(
      std::bind(&receiving::onConnection, _1));
  server.setMessageCallback(
      std::bind(&receiving::onMessage, _1, _2, _3));
  server.start();
  loop.loop();
}

```

vim显示行数：`:set nu`

[非阻塞IO实现的](muduo\examples\ace\ttcp\ttcp.cc)，会降低性能。

## 测试环境

在那些机器测试：客户端的机器，服务器的机器。用什么机器运行。三维的数据。还有一个是tcp的消息个数n和长度l(1024,2048...65536)，可以让两个相乘为一个定长。

ttcp和延迟有关。nc只和带宽有关，和延迟无关。

相同带宽下延迟不同，测出来的性能也不一样。

## 方法

在一个机器运行客户端`while true; do ./ttcp -r; done`

一个机器扮演服务端，往客户端机器发送长度。`./ttcp -t xx -l 1024`

消息发的越小，传输延迟的影响就越大。

测试在本机：`./ttcp -t localhost -l 4096`

作为进程间通信，tcp在本机也很强。

## 阻塞IO下的ttcp实验

每个线程对应一个客户端，一个回显服务。

看发送多少会阻塞。`./echo_client localhost 20240000`

查看连接的状态`netstat -tpn | grep '3007\|^[AP]'`

发现服务器和客户端的接收队列上有多少数据，发送队列有多少数据。

cli->(send数据进入内核缓冲区)->ser(recv->send，服务器等待客户端读，阻塞在send上，客户端在发完之前不会读，客户端就阻塞在send上了。)->cli(recv)

发生长期阻塞无法自我解除。

查看内核参数`sysctl -A |grep tcp.*mem`

查看本机端口号：`sysctl -A |grep range`

设计协议的时候，客户端的header告诉消息有多大，服务器准备一个这么大的缓冲区接收请求，开始计算res，发回客户端。

## TCP自连接

```python
import errno
import socket
import sys
import time

if len(sys.argv) < 2:
    print "Usage: %s port" % sys.argv[0]
    print "port should in net.ipv4.ip_local_port_range"
else:
    port = int(sys.argv[1])
    for i in range(65536):
        try:
            sock = socket.create_connection(('localhost', port))
            # 打印本机地址和对方地址
            print "connected", sock.getsockname(), sock.getpeername()
            time.sleep(60*60)
        except socket.error, e:
            if e.errno != errno.ECONNREFUSED:
                break
```

查看本机有哪些端口在监听：`netstat -ltnp`

tcp在发起一个连接时，会选择一个临时端口号。选定后往服务器的端口发请求。需要和服务器监听的端口不一样。如果是同一个端口，就是自连接。tcp的同时打开功能。

防止：可以在连接成功后判断一下是不是自连接，如果是就断开。就是本机地址是否等于对方地址。

## 扩展

测试消息本身的吞吐量。消息大小与消息吞吐量的关系表。测试延迟。

支持并发客户。如果是阻塞，每个客户端开启一个线程。如果是非阻塞，用事件循环，用epoll。

测试tcp公平性。多个客户端往同一个服务器发送，看分配给每个客户端的带宽是不是一样大。

延迟大带宽大的用多个连接把带宽跑满，或者用一个连接采用流水线(同一时间有多个消息在发数据。如果消息数增加到一定程度会阻塞)把带宽跑满。

观察慢启动。消息数目和带宽的关系。

验证消息内容是否出错。

# Roundtrip

基于udp

## 时钟概述

A clock = An oscillator + a counter

主要改进震荡性。

current time = count/frequency + offset 准确度和稳定度决定时钟好坏

## 时钟精确度和校准

精确&准确

频率随时间的变化

测试：当前时间(T*)&时间间隔(size T)

tsc

## 网络时间同步

**NTP**

基于udp。三个时间戳：客户端发出，服务端收到，服务端收到。

可以算出时间差((t1+t4) - (t2+t3)) / 2

调整本机时间和服务器之间的差值，频率，连续调整(避免时钟跳变)。

避免频率跳变。

选定固定的服务器。

机器收到距离进程收到之间有微秒级的延迟。发送的时候只能知道用户态发出的时间，不知道网卡发出的时间。NTP固有的误差。

**测量时间差**

两台机器同步到不同的ntp server。无法知道两台机器时间差多少。如何测量。请求响应不需要测，是同一个机器的。

消息系统：股票交易所->(股票价格)->引擎(可以总和其他指标)->客户接入的服务->客户交易软件

测量股票交易所发送到客户的时间。

[代码](E:\读书笔记\muduo\examples\roundtrip)

往返：t3 - t1

误差：t2 - (t1 + t3) / 2

消息格式：16字节，分两块。低位在前。LE。t1, t2.客户端发过去填写t1，t2空着。服务端收到后填写t2，拷贝t1，发给客户端。客户端收到后取当前时间得到t3.为了对称性。两边都是发8字节。

## 其他测试方案

冒烟测试：看基本正确性。同一台机器&两台机器。

两台机器都没有ntp。free running。差距会线性改变。

一个 ntp，一个没有。会是二次曲线。

两台机器都有ntp，但是同步在不同的ntp

两个都有ntp，同步在同一个ntp服务器。

两个都有ntp，以其中一个为另一个的ntp服务器。可以测ntp本身的性能。

实际部署的时候，一般一个机房有4台ntp服务器。是第一层。两个是同步到gps的。

# UDP & TCP

udp：ip + port

何时用udp：受限情况下。

1. 你真的很在乎延迟，不能忍受重传，那么就用UDP，例如 NTP 协议。重传NTP消息纯属添乱。
2. 你真的不在乎可靠性，丢一些包也不需要重传，那么就可以用 UDP。例子我想不出来。有人说音频或视频流可以用UDP，不过据我看来各大视频网站都用HTTP协议，而HTTP是基于TCP的。
3. 你需要NAT穿透，那么不得不用UDP。
4. 其他情况，一旦程序要自己做重传，你都是在用UDP模拟出蹩脚的TCP，还不如直接用TCP呢。

总之：使用 UDP 需要有强大到不容置疑的理由，when in doubt, use TCP.

一些协议，出于历史原因，受当时技术和网络条件限制，选择了基于UDP实现，其选择的理由现在很可能已经不再成立了。因此“xxx协议用UDP”不是你现在写网络应用程序也该用UDP的理由，除非你本身就是在实现xxx协议。

另外，那些说TCP比UDP慢、效率低的，你拿UDP写个程序，把千兆网带宽打满（TCP等价的代码只有两行：客户端 while (true) { send(...); } 服务端 while (true) { recv(...); }。），且不说你的程序会有多复杂，先看看goodput到底是不是比TCP大、CPU使用率是不是比TCP低嘛。

tcp一般一个线程读一个线程写、

udp所有客户端在服务端可以用一个线程。多个线程可以同时并发读写。

## netcat

除了要读取socket fd，还需要读取stdin, stdout。涉及并发。

Thread-per-connection with blocking IO

IO-multiplexing with non-blocking IO

如何安全关闭tcp连接

**nc**

nc < /dev/zero == chargen类似于信号发生器

nc/dev/null == discard

dd /dev/zero | nc == poor man's ttcp测试网络带宽

nc < file, nc > file == poor man's scp 在两个机器之间拷贝文件

难度：建立 < 销毁；服务器建立 < 客户端建立；接受 < 发送(非阻塞中)

**关闭连接**

close太早导致数据发送不完整。

发送方send后把写端关闭，超时后关闭。或者然后一直读数据，读到长度为0对方关闭连接后关闭。

接收方read返回0，并且没有其他要发送的数据，就close。

最好设计协议格式的时候把数据的长度包含。数据收到这么长的数据就断开连接。

## tcp使用注意事项

**SIGPIPE**

SIGPIPE:如果向已经关掉的管道写数据会收到这个信号。收到后终止进程。

例子：`gunzip -c huge.log.gz | grep ERROR | head`

取大文件前十行，收到十行以后，grep收到sigpipe, 随后gunzip也退出。

网络程序一般在进程启动时忽略sigpipe信号。

如果忽略了sigpipe,有个副作用，如果程序向stdout写数据，会检查printf的返回值然后exit

**TCP_NODELAY**

write-write-read，第二个write会延迟一个RTT。可以在应用层做缓冲，只发一次。但是如果同一个连接有多个请求，程序不同部分发请求，很难变成一个buffer。也就是request pipelining

建议默认关闭Nagle's algorithm

**SO_REUSEADDR**

如果有别的端口也监听80端口，会提示已经被用了。

## 实现netcat

服务端：bind+listen+accept

客户端：resolve address + connect

测试nc性能：chargen.cc

观察性能：

utop, nc, chargen.对比使用不同版本的nc

观察nc的吞吐量。观察utop可以知道cpu使用率，看是谁拖慢了性能。

## Thread-per-connection with blocking IO

每个连接对应两个线程。在go中用的很多。

主线程读stdin, 写socket.另一个线程读socket, 写stdout

进程退出条件：读stdin返回0.网络线程终止线程。读socket返回0.如果用IO复用就没问题了。

阻塞IO是自动限速的。

## IO-multiplexing

适用于连接不多，而且线程很廉价。

同步的。IO 复用复用的是线程。和非阻塞IO一起用。

# 非阻塞IO

